{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class IntentDataset(Dataset):\n",
    "\n",
    "  def __init__(self, csv_path, tokenizer, classes=None):\n",
    "    df = pd.read_csv(csv_path, sep=\";\")\n",
    "    if classes is None:\n",
    "        self.classes = sorted(df.intent.unique())\n",
    "    else:\n",
    "        self.classes = classes\n",
    "\n",
    "    self.id2class = {i: x for i, x in enumerate(self.classes)}\n",
    "    self.class2id = {x: i for i, x in enumerate(self.classes)}\n",
    "\n",
    "    self.labels = [self.class2id[x] for x in df.intent]\n",
    "    self.objects = list(df.text)\n",
    "    self.num_classes = len(self.classes)\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.objects)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    text = str(self.objects[i])\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'text': text,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(self.labels[i], dtype=torch.long)\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/magnus/Documents/MusicRocket/venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "\n",
    "train_set = IntentDataset(\"train.csv\", tokenizer)\n",
    "train_loader = DataLoader(train_set, batch_size=3, shuffle=True)\n",
    "\n",
    "valid_set = IntentDataset(\"test.csv\", tokenizer, train_set.classes)\n",
    "valid_loader = DataLoader(valid_set, batch_size=1)\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny\", num_labels=train_set.num_classes)\n",
    "model = model.cuda()\n",
    "model = model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(7, 47)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.num_classes, len(train_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train_epoch(model, device='cuda'):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        attention_mask = data[\"attention_mask\"].to(device)\n",
    "        targets = data[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        loss = loss_function(outputs.logits, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_acc = correct_predictions.double() / len(train_set)\n",
    "    train_loss = np.mean(losses)\n",
    "    return train_acc, train_loss\n",
    "\n",
    "\n",
    "def eval_epoch(model, device='cuda'):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            targets = data[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            loss = loss_function(outputs.logits, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    val_acc = correct_predictions.double() / len(valid_set)\n",
    "    val_loss = np.mean(losses)\n",
    "    return val_acc, val_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train loss 1.9179854094982147 accuracy 0.2127659574468085\n",
      "Val loss 1.8382936205182756 accuracy 0.2857142857142857\n",
      "----------\n",
      "Epoch 2/50\n",
      "Train loss 1.7026153728365898 accuracy 0.3829787234042553\n",
      "Val loss 1.6636668954576765 accuracy 0.2857142857142857\n",
      "----------\n",
      "Epoch 3/50\n",
      "Train loss 1.4779823049902916 accuracy 0.5106382978723404\n",
      "Val loss 1.5360872064317976 accuracy 0.2857142857142857\n",
      "----------\n",
      "Epoch 4/50\n",
      "Train loss 1.242090530693531 accuracy 0.6808510638297872\n",
      "Val loss 1.4106640688010625 accuracy 0.42857142857142855\n",
      "----------\n",
      "Epoch 5/50\n",
      "Train loss 1.043381068855524 accuracy 0.8936170212765957\n",
      "Val loss 1.3337423545973641 accuracy 0.5714285714285714\n",
      "----------\n",
      "Epoch 6/50\n",
      "Train loss 0.8901304490864277 accuracy 0.9574468085106382\n",
      "Val loss 1.165014433009284 accuracy 0.42857142857142855\n",
      "----------\n",
      "Epoch 7/50\n",
      "Train loss 0.7554843481630087 accuracy 1.0\n",
      "Val loss 1.0795178839138575 accuracy 0.5714285714285714\n",
      "----------\n",
      "Epoch 8/50\n",
      "Train loss 0.6312139257788658 accuracy 1.0\n",
      "Val loss 0.9928156988961356 accuracy 0.7142857142857142\n",
      "----------\n",
      "Epoch 9/50\n",
      "Train loss 0.5244271755218506 accuracy 1.0\n",
      "Val loss 0.9309379011392593 accuracy 0.7142857142857142\n",
      "----------\n",
      "Epoch 10/50\n",
      "Train loss 0.46476977691054344 accuracy 1.0\n",
      "Val loss 0.8608847239187786 accuracy 0.7142857142857142\n",
      "----------\n",
      "Epoch 11/50\n",
      "Train loss 0.3913457253947854 accuracy 1.0\n",
      "Val loss 0.8109841410602842 accuracy 0.5714285714285714\n",
      "----------\n",
      "Epoch 12/50\n",
      "Train loss 0.3413839042186737 accuracy 1.0\n",
      "Val loss 0.7412470430135727 accuracy 0.8571428571428571\n",
      "----------\n",
      "Epoch 13/50\n",
      "Train loss 0.3154015578329563 accuracy 1.0\n",
      "Val loss 0.7168787355933871 accuracy 0.7142857142857142\n",
      "----------\n",
      "Epoch 14/50\n",
      "Train loss 0.2819220321252942 accuracy 1.0\n",
      "Val loss 0.6746182388492993 accuracy 0.7142857142857142\n",
      "----------\n",
      "Epoch 15/50\n",
      "Train loss 0.26626814994961023 accuracy 1.0\n",
      "Val loss 0.6072584252272334 accuracy 0.7142857142857142\n",
      "----------\n",
      "Epoch 16/50\n",
      "Train loss 0.23662607837468386 accuracy 1.0\n",
      "Val loss 0.5814494756715638 accuracy 0.8571428571428571\n",
      "----------\n",
      "Epoch 17/50\n",
      "Train loss 0.22838986199349165 accuracy 1.0\n",
      "Val loss 0.5723871162959507 accuracy 1.0\n",
      "----------\n",
      "Epoch 18/50\n",
      "Train loss 0.19683228991925716 accuracy 1.0\n",
      "Val loss 0.5282187791807311 accuracy 1.0\n",
      "----------\n",
      "Epoch 19/50\n",
      "Train loss 0.19513936713337898 accuracy 1.0\n",
      "Val loss 0.4941409283450672 accuracy 1.0\n",
      "----------\n",
      "Epoch 20/50\n",
      "Train loss 0.18077627755701542 accuracy 1.0\n",
      "Val loss 0.4893054547054427 accuracy 1.0\n",
      "----------\n",
      "Epoch 21/50\n",
      "Train loss 0.17252814210951328 accuracy 1.0\n",
      "Val loss 0.47853137126990725 accuracy 1.0\n",
      "----------\n",
      "Epoch 22/50\n",
      "Train loss 0.16073684953153133 accuracy 1.0\n",
      "Val loss 0.4713551955563681 accuracy 1.0\n",
      "----------\n",
      "Epoch 23/50\n",
      "Train loss 0.1557132019661367 accuracy 1.0\n",
      "Val loss 0.4466056302189827 accuracy 1.0\n",
      "----------\n",
      "Epoch 24/50\n",
      "Train loss 0.1481408760882914 accuracy 1.0\n",
      "Val loss 0.43576483854225706 accuracy 1.0\n",
      "----------\n",
      "Epoch 25/50\n",
      "Train loss 0.14597343606874347 accuracy 1.0\n",
      "Val loss 0.42516401410102844 accuracy 1.0\n",
      "----------\n",
      "Epoch 26/50\n",
      "Train loss 0.13638225197792053 accuracy 1.0\n",
      "Val loss 0.42434010761124746 accuracy 1.0\n",
      "----------\n",
      "Epoch 27/50\n",
      "Train loss 0.13346583396196365 accuracy 1.0\n",
      "Val loss 0.4196640561733927 accuracy 1.0\n",
      "----------\n",
      "Epoch 28/50\n",
      "Train loss 0.13247392093762755 accuracy 1.0\n",
      "Val loss 0.40776695949690683 accuracy 1.0\n",
      "----------\n",
      "Epoch 29/50\n",
      "Train loss 0.1261803014203906 accuracy 1.0\n",
      "Val loss 0.391153413270201 accuracy 1.0\n",
      "----------\n",
      "Epoch 30/50\n",
      "Train loss 0.12222157930955291 accuracy 1.0\n",
      "Val loss 0.37312980900917736 accuracy 1.0\n",
      "----------\n",
      "Epoch 31/50\n",
      "Train loss 0.11749963136389852 accuracy 1.0\n",
      "Val loss 0.3574429244867393 accuracy 1.0\n",
      "----------\n",
      "Epoch 32/50\n",
      "Train loss 0.11639423435553908 accuracy 1.0\n",
      "Val loss 0.35664992832711767 accuracy 1.0\n",
      "----------\n",
      "Epoch 33/50\n",
      "Train loss 0.11488938331604004 accuracy 1.0\n",
      "Val loss 0.3591054784400122 accuracy 1.0\n",
      "----------\n",
      "Epoch 34/50\n",
      "Train loss 0.11261320626363158 accuracy 1.0\n",
      "Val loss 0.36439603354249683 accuracy 1.0\n",
      "----------\n",
      "Epoch 35/50\n",
      "Train loss 0.10809279093518853 accuracy 1.0\n",
      "Val loss 0.36522926070860456 accuracy 1.0\n",
      "----------\n",
      "Epoch 36/50\n",
      "Train loss 0.10680904425680637 accuracy 1.0\n",
      "Val loss 0.35376218759587835 accuracy 1.0\n",
      "----------\n",
      "Epoch 37/50\n",
      "Train loss 0.10499713942408562 accuracy 1.0\n",
      "Val loss 0.35121067453707966 accuracy 1.0\n",
      "----------\n",
      "Epoch 38/50\n",
      "Train loss 0.10529272072017193 accuracy 1.0\n",
      "Val loss 0.35259529895016123 accuracy 1.0\n",
      "----------\n",
      "Epoch 39/50\n",
      "Train loss 0.10179360071197152 accuracy 1.0\n",
      "Val loss 0.34842426595943315 accuracy 1.0\n",
      "----------\n",
      "Epoch 40/50\n",
      "Train loss 0.10271137580275536 accuracy 1.0\n",
      "Val loss 0.348293170865093 accuracy 1.0\n",
      "----------\n",
      "Epoch 41/50\n",
      "Train loss 0.10060905106365681 accuracy 1.0\n",
      "Val loss 0.34782890709383146 accuracy 1.0\n",
      "----------\n",
      "Epoch 42/50\n",
      "Train loss 0.09822538588196039 accuracy 1.0\n",
      "Val loss 0.3450100038732801 accuracy 1.0\n",
      "----------\n",
      "Epoch 43/50\n",
      "Train loss 0.0992704932577908 accuracy 1.0\n",
      "Val loss 0.34219622558781077 accuracy 1.0\n",
      "----------\n",
      "Epoch 44/50\n",
      "Train loss 0.09965096320956945 accuracy 1.0\n",
      "Val loss 0.339116315224341 accuracy 1.0\n",
      "----------\n",
      "Epoch 45/50\n",
      "Train loss 0.09569565439596772 accuracy 1.0\n",
      "Val loss 0.33701892676098005 accuracy 1.0\n",
      "----------\n",
      "Epoch 46/50\n",
      "Train loss 0.09950903244316578 accuracy 1.0\n",
      "Val loss 0.33320065640977453 accuracy 1.0\n",
      "----------\n",
      "Epoch 47/50\n",
      "Train loss 0.09738091821782291 accuracy 1.0\n",
      "Val loss 0.3302817775734833 accuracy 1.0\n",
      "----------\n",
      "Epoch 48/50\n",
      "Train loss 0.09519685315899551 accuracy 1.0\n",
      "Val loss 0.32831221393176485 accuracy 1.0\n",
      "----------\n",
      "Epoch 49/50\n",
      "Train loss 0.09398750588297844 accuracy 1.0\n",
      "Val loss 0.32717341503926683 accuracy 1.0\n",
      "----------\n",
      "Epoch 50/50\n",
      "Train loss 0.09226776333525777 accuracy 1.0\n",
      "Val loss 0.32683456050498144 accuracy 1.0\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    model = model.train()\n",
    "    train_acc, train_loss = train_epoch(model)\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    model = model.eval()\n",
    "    val_acc, val_loss = eval_epoch(model)\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "    print('-' * 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "model.id2class = train_set.id2class\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "def inference(text, device='cuda'):\n",
    "    encoding = model.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask).logits.detach().cpu()\n",
    "\n",
    "    return model.id2class[torch.argmax(out, dim=1).item()]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нужно сделать аринжировку arrangement\n",
      "У меня есть тексти и ноты, можно у вас записать вокал? recording\n",
      "Хотим приятный дизайн обложки album_cover\n",
      "Хотим чтобы новый трек был на радио promotion\n",
      "Есть хорошая демка, хочу в скором времени выпустить её на спотифай release\n",
      "Почему так дорого trade\n",
      "Добрый день other\n"
     ]
    }
   ],
   "source": [
    "for i in valid_set:\n",
    "    print(i['text'], inference(i['text']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "torch.save(model, r\"weights/best.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
