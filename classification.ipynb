{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class IntentDataset(Dataset):\n",
    "\n",
    "  def __init__(self, csv_path, tokenizer, classes=None):\n",
    "    df = pd.read_csv(csv_path, sep=\";\")\n",
    "    if classes is None:\n",
    "        self.classes = sorted(df.intent.unique())\n",
    "    else:\n",
    "        self.classes = classes\n",
    "\n",
    "    self.id2class = {i: x for i, x in enumerate(self.classes)}\n",
    "    self.class2id = {x: i for i, x in enumerate(self.classes)}\n",
    "\n",
    "    self.labels = [self.class2id[x] for x in df.intent]\n",
    "    self.objects = list(df.text)\n",
    "    self.num_classes = len(self.classes)\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.objects)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    text = str(self.objects[i])\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'text': text,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(self.labels[i], dtype=torch.long)\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/magnus/Documents/MusicRocket/venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "\n",
    "train_set = IntentDataset(os.path.join(classification_dataset, \"train.csv\"), tokenizer)\n",
    "train_loader = DataLoader(train_set, batch_size=3, shuffle=True)\n",
    "\n",
    "valid_set = IntentDataset(os.path.join(classification_dataset, \"test.csv\"), tokenizer, train_set.classes)\n",
    "valid_loader = DataLoader(valid_set, batch_size=1)\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny\", num_labels=train_set.num_classes)\n",
    "model = model.cuda()\n",
    "model = model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(8, 49)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.num_classes, len(train_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def train_epoch(model, device='cuda'):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        attention_mask = data[\"attention_mask\"].to(device)\n",
    "        targets = data[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        loss = loss_function(outputs.logits, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_acc = correct_predictions.double() / len(train_set)\n",
    "    train_loss = np.mean(losses)\n",
    "    return train_acc, train_loss\n",
    "\n",
    "\n",
    "def eval_epoch(model, device='cuda'):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            targets = data[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            loss = loss_function(outputs.logits, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    val_acc = correct_predictions.double() / len(valid_set)\n",
    "    val_loss = np.mean(losses)\n",
    "    return val_acc, val_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train loss 2.0516065429238712 accuracy 0.22448979591836732\n",
      "Val loss 1.966776818037033 accuracy 0.25\n",
      "----------\n",
      "Epoch 2/50\n",
      "Train loss 1.8335944063523237 accuracy 0.5102040816326531\n",
      "Val loss 1.8056159019470215 accuracy 0.25\n",
      "----------\n",
      "Epoch 3/50\n",
      "Train loss 1.5870801561019 accuracy 0.5918367346938775\n",
      "Val loss 1.6328246891498566 accuracy 0.25\n",
      "----------\n",
      "Epoch 4/50\n",
      "Train loss 1.309303458999185 accuracy 0.7551020408163265\n",
      "Val loss 1.4723696410655975 accuracy 0.625\n",
      "----------\n",
      "Epoch 5/50\n",
      "Train loss 1.1292449937147253 accuracy 0.8571428571428571\n",
      "Val loss 1.2839204296469688 accuracy 0.75\n",
      "----------\n",
      "Epoch 6/50\n",
      "Train loss 0.9249447619213778 accuracy 0.9387755102040816\n",
      "Val loss 1.1286193877458572 accuracy 0.75\n",
      "----------\n",
      "Epoch 7/50\n",
      "Train loss 0.784454065210679 accuracy 0.9591836734693877\n",
      "Val loss 0.9912850707769394 accuracy 0.75\n",
      "----------\n",
      "Epoch 8/50\n",
      "Train loss 0.6463715749628404 accuracy 0.9999999999999999\n",
      "Val loss 0.8667638227343559 accuracy 0.875\n",
      "----------\n",
      "Epoch 9/50\n",
      "Train loss 0.5499529926215901 accuracy 0.9999999999999999\n",
      "Val loss 0.8015339523553848 accuracy 0.875\n",
      "----------\n",
      "Epoch 10/50\n",
      "Train loss 0.48385659035514383 accuracy 0.9999999999999999\n",
      "Val loss 0.7199240103363991 accuracy 0.875\n",
      "----------\n",
      "Epoch 11/50\n",
      "Train loss 0.41780130477512584 accuracy 0.9999999999999999\n",
      "Val loss 0.6717542633414268 accuracy 0.875\n",
      "----------\n",
      "Epoch 12/50\n",
      "Train loss 0.38895324047874 accuracy 0.9999999999999999\n",
      "Val loss 0.6281424984335899 accuracy 1.0\n",
      "----------\n",
      "Epoch 13/50\n",
      "Train loss 0.34237925270024466 accuracy 0.9999999999999999\n",
      "Val loss 0.5784338600933552 accuracy 1.0\n",
      "----------\n",
      "Epoch 14/50\n",
      "Train loss 0.3123174546396031 accuracy 0.9999999999999999\n",
      "Val loss 0.5613883957266808 accuracy 0.875\n",
      "----------\n",
      "Epoch 15/50\n",
      "Train loss 0.28686806296601014 accuracy 0.9999999999999999\n",
      "Val loss 0.5260376650840044 accuracy 1.0\n",
      "----------\n",
      "Epoch 16/50\n",
      "Train loss 0.26895658934817596 accuracy 0.9999999999999999\n",
      "Val loss 0.502121027559042 accuracy 1.0\n",
      "----------\n",
      "Epoch 17/50\n",
      "Train loss 0.2515440863721511 accuracy 0.9999999999999999\n",
      "Val loss 0.4680970348417759 accuracy 1.0\n",
      "----------\n",
      "Epoch 18/50\n",
      "Train loss 0.22436110237065485 accuracy 0.9999999999999999\n",
      "Val loss 0.4612161014229059 accuracy 1.0\n",
      "----------\n",
      "Epoch 19/50\n",
      "Train loss 0.2145479084814296 accuracy 0.9999999999999999\n",
      "Val loss 0.44605742394924164 accuracy 1.0\n",
      "----------\n",
      "Epoch 20/50\n",
      "Train loss 0.21125324158107534 accuracy 0.9999999999999999\n",
      "Val loss 0.4267945159226656 accuracy 1.0\n",
      "----------\n",
      "Epoch 21/50\n",
      "Train loss 0.19527601845124187 accuracy 0.9999999999999999\n",
      "Val loss 0.40685958229005337 accuracy 1.0\n",
      "----------\n",
      "Epoch 22/50\n",
      "Train loss 0.19016474573051229 accuracy 0.9999999999999999\n",
      "Val loss 0.3937242142856121 accuracy 1.0\n",
      "----------\n",
      "Epoch 23/50\n",
      "Train loss 0.17835970047642202 accuracy 0.9999999999999999\n",
      "Val loss 0.38390287943184376 accuracy 1.0\n",
      "----------\n",
      "Epoch 24/50\n",
      "Train loss 0.17873866680790396 accuracy 0.9999999999999999\n",
      "Val loss 0.378247519955039 accuracy 1.0\n",
      "----------\n",
      "Epoch 25/50\n",
      "Train loss 0.16417264456258102 accuracy 0.9999999999999999\n",
      "Val loss 0.36376085318624973 accuracy 1.0\n",
      "----------\n",
      "Epoch 26/50\n",
      "Train loss 0.16080689386409872 accuracy 0.9999999999999999\n",
      "Val loss 0.35737407952547073 accuracy 1.0\n",
      "----------\n",
      "Epoch 27/50\n",
      "Train loss 0.15093704356866725 accuracy 0.9999999999999999\n",
      "Val loss 0.3473400743678212 accuracy 1.0\n",
      "----------\n",
      "Epoch 28/50\n",
      "Train loss 0.14946971921359792 accuracy 0.9999999999999999\n",
      "Val loss 0.33344436809420586 accuracy 1.0\n",
      "----------\n",
      "Epoch 29/50\n",
      "Train loss 0.14541072573731928 accuracy 0.9999999999999999\n",
      "Val loss 0.3297383086755872 accuracy 1.0\n",
      "----------\n",
      "Epoch 30/50\n",
      "Train loss 0.14484609882621205 accuracy 0.9999999999999999\n",
      "Val loss 0.32773563638329506 accuracy 1.0\n",
      "----------\n",
      "Epoch 31/50\n",
      "Train loss 0.13683313131332397 accuracy 0.9999999999999999\n",
      "Val loss 0.3175874901935458 accuracy 1.0\n",
      "----------\n",
      "Epoch 32/50\n",
      "Train loss 0.13302848400438533 accuracy 0.9999999999999999\n",
      "Val loss 0.31128271110355854 accuracy 1.0\n",
      "----------\n",
      "Epoch 33/50\n",
      "Train loss 0.13091547611881704 accuracy 0.9999999999999999\n",
      "Val loss 0.30858086235821247 accuracy 1.0\n",
      "----------\n",
      "Epoch 34/50\n",
      "Train loss 0.13493452615597668 accuracy 0.9999999999999999\n",
      "Val loss 0.3082856209948659 accuracy 1.0\n",
      "----------\n",
      "Epoch 35/50\n",
      "Train loss 0.13103138041846893 accuracy 0.9999999999999999\n",
      "Val loss 0.30328064784407616 accuracy 1.0\n",
      "----------\n",
      "Epoch 36/50\n",
      "Train loss 0.1291687028372989 accuracy 0.9999999999999999\n",
      "Val loss 0.29669832717627287 accuracy 1.0\n",
      "----------\n",
      "Epoch 37/50\n",
      "Train loss 0.12420126182191513 accuracy 0.9999999999999999\n",
      "Val loss 0.2921865927055478 accuracy 1.0\n",
      "----------\n",
      "Epoch 38/50\n",
      "Train loss 0.12104103363612119 accuracy 0.9999999999999999\n",
      "Val loss 0.28835941571742296 accuracy 1.0\n",
      "----------\n",
      "Epoch 39/50\n",
      "Train loss 0.1221530910800485 accuracy 0.9999999999999999\n",
      "Val loss 0.2843620516359806 accuracy 1.0\n",
      "----------\n",
      "Epoch 40/50\n",
      "Train loss 0.11512406477156807 accuracy 0.9999999999999999\n",
      "Val loss 0.2823401279747486 accuracy 1.0\n",
      "----------\n",
      "Epoch 41/50\n",
      "Train loss 0.12454372951213051 accuracy 0.9999999999999999\n",
      "Val loss 0.27837278321385384 accuracy 1.0\n",
      "----------\n",
      "Epoch 42/50\n",
      "Train loss 0.11515436277670019 accuracy 0.9999999999999999\n",
      "Val loss 0.27354683447629213 accuracy 1.0\n",
      "----------\n",
      "Epoch 43/50\n",
      "Train loss 0.11804395914077759 accuracy 0.9999999999999999\n",
      "Val loss 0.27150458935648203 accuracy 1.0\n",
      "----------\n",
      "Epoch 44/50\n",
      "Train loss 0.11406828112461988 accuracy 0.9999999999999999\n",
      "Val loss 0.2692546136677265 accuracy 1.0\n",
      "----------\n",
      "Epoch 45/50\n",
      "Train loss 0.11258606437374563 accuracy 0.9999999999999999\n",
      "Val loss 0.2680154852569103 accuracy 1.0\n",
      "----------\n",
      "Epoch 46/50\n",
      "Train loss 0.11309413392754163 accuracy 0.9999999999999999\n",
      "Val loss 0.26763517037034035 accuracy 1.0\n",
      "----------\n",
      "Epoch 47/50\n",
      "Train loss 0.11593875376617208 accuracy 0.9999999999999999\n",
      "Val loss 0.2670872090384364 accuracy 1.0\n",
      "----------\n",
      "Epoch 48/50\n",
      "Train loss 0.11074251287123736 accuracy 0.9999999999999999\n",
      "Val loss 0.2661126907914877 accuracy 1.0\n",
      "----------\n",
      "Epoch 49/50\n",
      "Train loss 0.11159617541467443 accuracy 0.9999999999999999\n",
      "Val loss 0.26533767208456993 accuracy 1.0\n",
      "----------\n",
      "Epoch 50/50\n",
      "Train loss 0.10650322542471044 accuracy 0.9999999999999999\n",
      "Val loss 0.2652897695079446 accuracy 1.0\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    model = model.train()\n",
    "    train_acc, train_loss = train_epoch(model)\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    model = model.eval()\n",
    "    val_acc, val_loss = eval_epoch(model)\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "    print('-' * 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "model.id2class = train_set.id2class\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "def inference(text, device='cuda'):\n",
    "    encoding = model.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask).logits.detach().cpu()\n",
    "\n",
    "    return model.id2class[torch.argmax(out, dim=1).item()]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нужно сделать аринжировку arrangement\n",
      "У меня есть тексти и ноты, можно у вас записать вокал? recording\n",
      "Хотим приятный дизайн обложки album_cover\n",
      "Хотим чтобы новый трек был на радио promotion\n",
      "Есть хорошая демка, хочу в скором времени выпустить её на спотифай release\n",
      "Почему так дорого trade\n",
      "Добрый день other\n",
      "Как тебя зовут? name\n"
     ]
    }
   ],
   "source": [
    "for i in valid_set:\n",
    "    print(i['text'], inference(i['text']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "torch.save(model, r\"weights/best.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
